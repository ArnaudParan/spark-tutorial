{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4dc1bb",
   "metadata": {},
   "source": [
    "# Pokedex DataFrame\n",
    "\n",
    "the dataframe API in spark is a dynamically typed API which allows pandas/SQL\n",
    "like requests in a distributed (and possibly noisy) environment.\n",
    "It compiles to RDD operations on the low level, but the high level is\n",
    "pretty straightforward.\n",
    "\n",
    "Most of the time, you will only need to use that API, however, sometimes you will\n",
    "need statically typed code and some other times (typically on custom aggregations)\n",
    "you will need to create the low level RDD code yourself. Most of the time though,\n",
    "you will use DataFrames and DataSets\n",
    "\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ebe2f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{functions => F}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Your spark session has already been created\n",
    "\n",
    "val df = spark\n",
    "    .read\n",
    "    .option(\"multiline\", true)\n",
    "    .option(\"header\", true)\n",
    "    .format(\"csv\")\n",
    "    .load(\"pokedex_(Update_05.20).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c61cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "    .select(\"name\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8327c",
   "metadata": {},
   "source": [
    "## Correcting the types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "    .select(\"_c0\")\n",
    "    .dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "// check the types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "// correct the types using withColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8f9ddc",
   "metadata": {},
   "source": [
    "## Analyse the data\n",
    "\n",
    "Which pokemons are the highest?\n",
    "\n",
    "Which pokemons are the heaviest?\n",
    "\n",
    "Which pokemons have the biggest BMI (mass / heightÂ²)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "// compute max and select when the value is the max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c5433",
   "metadata": {},
   "source": [
    "## Group By and Join\n",
    "\n",
    "Which pokemons are the heaviest in each generation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9578b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "// compute the groupby aggregations and inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9aa226",
   "metadata": {},
   "source": [
    "# Pokedex Dataset\n",
    "\n",
    "DataSets are the statically typed equivalent of DataFrames, when working\n",
    "with datasets, all the types are checked by the Scala compiler, which means\n",
    "that you will be able to spot mistakes earlier in your process\n",
    "\n",
    "DataFrames are DataSets with type Row, DataFrame = DataSet\\[Row\\]\n",
    "and the type row can take data of any schema\n",
    "\n",
    "Compute the maximum hp using the typed Dataset API with map and reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "// create the case class corresponding to the row types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e49eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "// load the data and cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c37cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "    .map(_.hp)\n",
    "    .reduce((x: Integer, y: Integer) => new Integer(Integer.max(x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f487ad80",
   "metadata": {},
   "source": [
    "# RDD API\n",
    "\n",
    "The RDD API is low level and most of the time you won't need to use it.\n",
    "However, in som cases, your Spark jobs might fail because you have a lot of\n",
    "data and you need to optimise your code if you don't want the cluster to\n",
    "give up on your jobs, so it is useful to know how to use RDDs.\n",
    "\n",
    "get the total weight of each generation of pokemons with the RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12736cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val dfRDD: RDD[(String, Float)] = df\n",
    "    .select(\"generation\", \"weight_kg\")\n",
    "    .as[(String, Float)]\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRDD\n",
    "    .reduceByKey(_+_)\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dd16cf",
   "metadata": {},
   "source": [
    "# TF IDF IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark\n",
    "    .read\n",
    "    .option(\"multiline\", true)\n",
    "    .option(\"header\", true)\n",
    "    .format(\"csv\")\n",
    "    .load(\"IMDB Dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad9e1f",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f790a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "\n",
    "val tkn = new Tokenizer()\n",
    "    .setInputCol(\"review\")\n",
    "    .setOutputCol(\"review_toks\")\n",
    "val tokenized = tkn.transform(df)\n",
    "tokenized.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e15d3",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26752f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "val englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "val stops = new StopWordsRemover()\n",
    "    .setStopWords(englishStopWords)\n",
    "    .setInputCol(\"review_toks\")\n",
    "    .setOutputCol(\"review_toks_sw\")\n",
    "\n",
    "val sw = stops.transform(tokenized)\n",
    "\n",
    "sw.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf2f1b",
   "metadata": {},
   "source": [
    "## Computing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c211f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
    "\n",
    "\n",
    "val tf = new HashingTF()\n",
    "    .setInputCol(\"review_toks_sw\")\n",
    "    .setOutputCol(\"TFOut\")\n",
    "    .setNumFeatures(10000)\n",
    "\n",
    "val idf = new IDF()\n",
    "    .setInputCol(\"TFOut\")\n",
    "    .setOutputCol(\"IDFOut\")\n",
    "    .setMinDocFreq(2)\n",
    "\n",
    "val tfdat = tf.transform(sw)\n",
    "\n",
    "val tfIdfOut = idf.fit(tfdat)\n",
    "    .transform(tfdat)\n",
    "\n",
    "tfIdfOut.select(\"TFOut\", \"IDFOut\").show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
