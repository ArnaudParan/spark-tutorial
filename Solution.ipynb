{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0b9f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4040\n",
       "SparkContext available as 'sc' (version = 3.0.2, master = local[*], app id = local-1623684744015)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2b845953\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e170b2",
   "metadata": {},
   "source": [
    "# Pokedex DataFrame\n",
    "\n",
    "\n",
    "\n",
    "the dataframe API in spark is a dynamically typed API which allows pandas/SQL\n",
    "like requests in a distributed (and possibly noisy) environment.\n",
    "It compiles to RDD operations on the low level, but the high level is\n",
    "pretty straightforward.\n",
    "\n",
    "Most of the time, you will only need to use that API, however, sometimes you will\n",
    "need statically typed code and some other times (typically on custom aggregations)\n",
    "you will need to create the low level RDD code yourself. Most of the time though,\n",
    "you will use DataFrames and DataSets\n",
    "\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640ce0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{functions=>F}\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{functions => F}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66e2901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [_c0: string, pokedex_number: string ... 49 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Your spark session has already been created\n",
    "\n",
    "val df = spark\n",
    "    .read\n",
    "    .option(\"multiline\", true)\n",
    "    .option(\"header\", true)\n",
    "    .format(\"csv\")\n",
    "    .load(\"pokedex_(Update_05.20).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c61cdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[String] = Array(_c0, pokedex_number, name, german_name, japanese_name, generation, status, species, type_number, type_1, type_2, height_m, weight_kg, abilities_number, ability_1, ability_2, ability_hidden, total_points, hp, attack, defense, sp_attack, sp_defense, speed, catch_rate, base_friendship, base_experience, growth_rate, egg_type_number, egg_type_1, egg_type_2, percentage_male, egg_cycles, against_normal, against_fire, against_water, against_electric, against_grass, against_ice, against_fight, against_poison, against_ground, against_flying, against_psychic, against_bug, against_rock, against_ghost, against_dragon, against_dark, against_steel, \"against_fairy\r",
       "\")\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ca86e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         name|\n",
      "+-------------+\n",
      "|    Bulbasaur|\n",
      "|      Ivysaur|\n",
      "|     Venusaur|\n",
      "|Mega Venusaur|\n",
      "|   Charmander|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "    .select(\"name\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6afb9e",
   "metadata": {},
   "source": [
    "## Correcting the types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ace98d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[(String, String)] = Array((_c0,StringType), (generation,StringType), (status,StringType), (height_m,StringType), (weight_kg,StringType), (speed,StringType))\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "    .select(\"_c0\", \"generation\", \"status\", \"height_m\", \"weight_kg\", \"speed\")\n",
    "    .dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b6e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+--------+---------+-----+\n",
      "|_c0|generation|status|height_m|weight_kg|speed|\n",
      "+---+----------+------+--------+---------+-----+\n",
      "|  0|         1|Normal|     0.7|      6.9|   45|\n",
      "|  1|         1|Normal|     1.0|     13.0|   60|\n",
      "|  2|         1|Normal|     2.0|    100.0|   80|\n",
      "|  3|         1|Normal|     2.4|    155.5|   80|\n",
      "|  4|         1|Normal|     0.6|      8.5|   65|\n",
      "+---+----------+------+--------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "    .select(\"_c0\", \"generation\", \"status\", \"height_m\", \"weight_kg\", \"speed\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e04666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------+-------+---------+----------+-----+----------+---------------+---------------+\n",
      "|total_points| hp|attack|defense|sp_attack|sp_defense|speed|catch_rate|base_friendship|base_experience|\n",
      "+------------+---+------+-------+---------+----------+-----+----------+---------------+---------------+\n",
      "|         318| 45|    49|     49|       65|        65|   45|        45|             70|             64|\n",
      "|         405| 60|    62|     63|       80|        80|   60|        45|             70|            142|\n",
      "|         525| 80|    82|     83|      100|       100|   80|        45|             70|            236|\n",
      "|         625| 80|   100|    123|      122|       120|   80|        45|             70|            281|\n",
      "|         309| 39|    52|     43|       60|        50|   65|        45|             70|             62|\n",
      "+------------+---+------+-------+---------+----------+-----+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "    .select(\"total_points\", \"hp\", \"attack\", \"defense\", \"sp_attack\", \"sp_defense\",\n",
    "          \"speed\", \"catch_rate\", \"base_friendship\", \"base_experience\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "309c9998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|percentage_male|\n",
      "+---------------+\n",
      "|           87.5|\n",
      "|           87.5|\n",
      "|           87.5|\n",
      "|           87.5|\n",
      "|           87.5|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "    .select(\"percentage_male\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f21a8010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [_c0: int, pokedex_number: int ... 49 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    "    .read\n",
    "    .option(\"multiline\", true)\n",
    "    .option(\"header\", true)\n",
    "    .format(\"csv\")\n",
    "    .load(\"pokedex_(Update_05.20).csv\")\n",
    "    .withColumn(\"height_m\",        col(\"height_m\").cast(\"float\"))\n",
    "    .withColumn(\"weight_kg\",       $\"weight_kg\".cast(\"float\"))\n",
    "    .withColumn(\"percentage_male\", $\"percentage_male\".cast(\"float\"))\n",
    "    .withColumn(\"_c0\",             $\"_c0\".cast(\"int\"))\n",
    "    .withColumn(\"pokedex_number\",  $\"pokedex_number\".cast(\"int\"))\n",
    "    .withColumn(\"total_points\",    $\"total_points\".cast(\"int\"))\n",
    "    .withColumn(\"hp\",              $\"hp\".cast(\"int\"))\n",
    "    .withColumn(\"attack\",          $\"attack\".cast(\"int\"))\n",
    "    .withColumn(\"defense\",         $\"defense\".cast(\"int\"))\n",
    "    .withColumn(\"sp_attack\",       $\"sp_attack\".cast(\"int\"))\n",
    "    .withColumn(\"sp_defense\",      $\"sp_defense\".cast(\"int\"))\n",
    "    .withColumn(\"speed\",           $\"speed\".cast(\"int\"))\n",
    "    .withColumn(\"catch_rate\",      $\"catch_rate\".cast(\"int\"))\n",
    "    .withColumn(\"base_friendship\", $\"base_friendship\".cast(\"int\"))\n",
    "    .withColumn(\"base_experience\", $\"base_experience\".cast(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc5f57",
   "metadata": {},
   "source": [
    "## Analyse the data\n",
    "\n",
    "Which pokemons are the highest?\n",
    "\n",
    "Which pokemons are the heaviest?\n",
    "\n",
    "Which pokemons have the biggest BMI (mass / heightÂ²)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6faff6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.sql.DataFrame = [max(height_m): float]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.select(F.max(df(\"height_m\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62716d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|               name|height_m|\n",
      "+-------------------+--------+\n",
      "|Eternatus Eternamax|   100.0|\n",
      "+-------------------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "max_height: Any = 100.0\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val max_height = df\n",
    "    .select(F.max(df(\"height_m\")))\n",
    "    .take(1)(0)(0)\n",
    "\n",
    "df\n",
    "    .filter(df(\"height_m\") === max_height)\n",
    "    .select(\"name\", \"height_m\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e63655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      name|weight_kg|\n",
      "+----------+---------+\n",
      "|   Cosmoem|    999.9|\n",
      "|Celesteela|    999.9|\n",
      "+----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "max_weight: Any = 999.9\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val max_weight = df\n",
    "    .select(F.max(df(\"weight_kg\")))\n",
    "    .take(1)(0)(0)\n",
    "\n",
    "df\n",
    "    .where(df(\"weight_kg\") === max_weight)\n",
    "    .select(\"name\", \"weight_kg\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77a77e87",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "27: error: overloaded method value select with alternatives:",
     "output_type": "error",
     "traceback": [
      "<console>:27: error: overloaded method value select with alternatives:",
      "  [U1, U2](c1: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U1], c2: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U2])org.apache.spark.sql.Dataset[(U1, U2)] <and>",
      "  (col: String,cols: String*)org.apache.spark.sql.DataFrame <and>",
      "  (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.DataFrame",
      " cannot be applied to (String, org.apache.spark.sql.Column)",
      "       val imc = df.select(\"name\", $\"height_m\" * $\"height_m\")",
      "                    ^",
      ""
     ]
    }
   ],
   "source": [
    "// that fails because you should not mix $ and \"\"\n",
    "\n",
    "val imc = df.select(\"name\", $\"height_m\" * $\"height_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbd948cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|               name|    imc|\n",
      "+-------------------+-------+\n",
      "|Eternatus Eternamax|10000.0|\n",
      "+-------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "imc: org.apache.spark.sql.DataFrame = [name: string, imc: float]\n",
       "max_imc: Any = 10000.0\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val imc = df.select($\"name\", ($\"height_m\" * $\"height_m\").as(\"imc\"))\n",
    "val max_imc = imc\n",
    "    .select(F.max($\"imc\"))\n",
    "    .take(1)(0)(0)\n",
    "\n",
    "imc\n",
    "    .where($\"imc\" === max_imc)\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d98cbe",
   "metadata": {},
   "source": [
    "## Group By and Join\n",
    "\n",
    "Which pokemons are the heaviest in each generation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e71eec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|generation|weight_kg|\n",
      "+----------+---------+\n",
      "|         7|    999.9|\n",
      "|         3|    999.7|\n",
      "|         8|    950.0|\n",
      "|         5|    345.0|\n",
      "|         6|    610.0|\n",
      "|         1|    460.0|\n",
      "|         4|    750.0|\n",
      "|         2|    740.0|\n",
      "+----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mass_by_gen: org.apache.spark.sql.DataFrame = [generation: string, weight_kg: float]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mass_by_gen = df\n",
    "    .groupBy(\"generation\")\n",
    "    .agg(F.max($\"weight_kg\").as(\"weight_kg\"))\n",
    "\n",
    "mass_by_gen.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2dfb3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+----------+---------+\n",
      "|generation|weight_kg|                name|generation|weight_kg|\n",
      "+----------+---------+--------------------+----------+---------+\n",
      "|         1|    460.0|             Snorlax|         1|    460.0|\n",
      "|         2|    740.0|        Mega Steelix|         2|    740.0|\n",
      "|         3|    999.7|      Primal Groudon|         3|    999.7|\n",
      "|         4|    750.0|Giratina Altered ...|         4|    750.0|\n",
      "|         5|    345.0|              Zekrom|         5|    345.0|\n",
      "|         6|    610.0|Zygarde Complete ...|         6|    610.0|\n",
      "|         7|    999.9|             Cosmoem|         7|    999.9|\n",
      "|         7|    999.9|          Celesteela|         7|    999.9|\n",
      "|         8|    950.0|           Eternatus|         8|    950.0|\n",
      "+----------+---------+--------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mass_by_gen\n",
    "    .join(\n",
    "        df.select(\"name\", \"generation\", \"weight_kg\"),\n",
    "        df(\"generation\") === mass_by_gen(\"generation\") && df(\"weight_kg\") === mass_by_gen(\"weight_kg\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f2d64",
   "metadata": {},
   "source": [
    "# Pokedex Dataset\n",
    "\n",
    "DataSets are the statically typed equivalent of DataFrames, when working\n",
    "with datasets, all the types are checked by the Scala compiler, which means\n",
    "that you will be able to spot mistakes earlier in your process\n",
    "\n",
    "DataFrames are DataSets with type Row, DataFrame = DataSet\\[Row\\]\n",
    "and the type row can take data of any schema\n",
    "\n",
    "Compute the maximum hp using the typed Dataset API with map and reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5aec7f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class PokedexRow\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class PokedexRow (\n",
    "    _c0: Integer,\n",
    "    pokedex_number: Integer,\n",
    "    name: String,\n",
    "    german_name: String,\n",
    "    japanese_name: String,\n",
    "    generation: String,\n",
    "    status: String,\n",
    "    species: String,\n",
    "    type_number: String,\n",
    "    type_1: String,\n",
    "    type_2: String,\n",
    "    height_m: Float,\n",
    "    weight_kg: Float,\n",
    "    abilities_number: String,\n",
    "    ability_1: String,\n",
    "    ability_2: String,\n",
    "    ability_hidden: String,\n",
    "    total_points: Integer,\n",
    "    hp: Integer,\n",
    "    attack: Integer,\n",
    "    defense: Integer,\n",
    "    sp_attack: Integer,\n",
    "    sp_defense: Integer,\n",
    "    speed: Integer,\n",
    "    catch_rate: Integer,\n",
    "    base_friendship: Integer,\n",
    "    base_experience: Integer,\n",
    "    growth_rate: String,\n",
    "    egg_type_number: String,\n",
    "    egg_type_1: String,\n",
    "    egg_type_2: String,\n",
    "    percentage_male: Float,\n",
    "    egg_cycles: String,\n",
    "    against_normal: String,\n",
    "    against_fire: String,\n",
    "    against_water: String,\n",
    "    against_electric: String,\n",
    "    against_grass: String,\n",
    "    against_ice: String,\n",
    "    against_fight: String,\n",
    "    against_poison: String,\n",
    "    against_ground: String,\n",
    "    against_flying: String,\n",
    "    against_psychic: String,\n",
    "    against_bug: String,\n",
    "    against_rock: String,\n",
    "    against_ghost: String,\n",
    "    against_dragon: String,\n",
    "    against_dark: String,\n",
    "    against_steel: String\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99e49eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.Dataset[PokedexRow] = [_c0: int, pokedex_number: int ... 49 more fields]\n",
       "res13: org.apache.spark.sql.Dataset[PokedexRow] = [_c0: int, pokedex_number: int ... 49 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    "    .read\n",
    "    .option(\"multiline\", true)\n",
    "    .option(\"header\", true)\n",
    "    .format(\"csv\")\n",
    "    .load(\"pokedex_(Update_05.20).csv\")\n",
    "    .withColumn(\"height_m\",        col(\"height_m\").cast(\"float\"))\n",
    "    .withColumn(\"weight_kg\",       $\"weight_kg\".cast(\"float\"))\n",
    "    .withColumn(\"percentage_male\", $\"percentage_male\".cast(\"float\"))\n",
    "    .withColumn(\"_c0\",             $\"_c0\".cast(\"int\"))\n",
    "    .withColumn(\"pokedex_number\",  $\"pokedex_number\".cast(\"int\"))\n",
    "    .withColumn(\"total_points\",    $\"total_points\".cast(\"int\"))\n",
    "    .withColumn(\"hp\",              $\"hp\".cast(\"int\"))\n",
    "    .withColumn(\"attack\",          $\"attack\".cast(\"int\"))\n",
    "    .withColumn(\"defense\",         $\"defense\".cast(\"int\"))\n",
    "    .withColumn(\"sp_attack\",       $\"sp_attack\".cast(\"int\"))\n",
    "    .withColumn(\"sp_defense\",      $\"sp_defense\".cast(\"int\"))\n",
    "    .withColumn(\"speed\",           $\"speed\".cast(\"int\"))\n",
    "    .withColumn(\"catch_rate\",      $\"catch_rate\".cast(\"int\"))\n",
    "    .withColumn(\"base_friendship\", $\"base_friendship\".cast(\"int\"))\n",
    "    .withColumn(\"base_experience\", $\"base_experience\".cast(\"int\"))\n",
    "    .na.drop\n",
    "    .as[PokedexRow]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2aa0f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Integer = 130\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "    .map(_.hp)\n",
    "    .reduce((x: Integer, y: Integer) => new Integer(Integer.max(x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f39876",
   "metadata": {},
   "source": [
    "# RDD API\n",
    "\n",
    "The RDD API is low level and most of the time you won't need to use it.\n",
    "However, in som cases, your Spark jobs might fail because you have a lot of\n",
    "data and you need to optimise your code if you don't want the cluster to\n",
    "give up on your jobs, so it is useful to know how to use RDDs.\n",
    "\n",
    "get the total weight of each generation of pokemons with the RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73796966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "dfRDD: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[87] at rdd at <console>:31\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val dfRDD: RDD[(String, Float)] = df\n",
    "    .select(\"generation\", \"weight_kg\")\n",
    "    .as[(String, Float)]\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3697a790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Array[(String, Float)] = Array((4,524.19995), (7,189.0), (5,414.30002), (6,497.0), (2,380.7), (3,777.0), (1,895.9))\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "You should use reduceByKey and not groupByKey\n",
    "because grouping will have to relocate data\n",
    "belonging in the same group to one cluster\n",
    "that might be quite intensive on the data\n",
    "while reduceByKey does not need that\n",
    "*/\n",
    "\n",
    "dfRDD\n",
    "    .reduceByKey(_+_)\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eddae5",
   "metadata": {},
   "source": [
    "# TF IDF IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7a27e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [review: string, sentiment: string]\n",
       "res16: org.apache.spark.sql.DataFrame = [review: string, sentiment: string]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    "    .read\n",
    "    .option(\"multiline\", true)\n",
    "    .option(\"header\", true)\n",
    "    .format(\"csv\")\n",
    "    .load(\"IMDB Dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2dc707",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c91802b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              review|           sentiment|         review_toks|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|One of the other ...|            positive|[one, of, the, ot...|\n",
      "|\"A wonderful litt...| not only is it w...|[\"a, wonderful, l...|\n",
      "|\"I thought this w...| but spirited you...|[\"i, thought, thi...|\n",
      "|Basically there's...|            negative|[basically, there...|\n",
      "|\"Petter Mattei's ...| power and succes...|[\"petter, mattei'...|\n",
      "|\"Probably my all-...| but that only ma...|[\"probably, my, a...|\n",
      "|I sure would like...|            positive|[i, sure, would, ...|\n",
      "|This show was an ...|            negative|[this, show, was,...|\n",
      "|Encouraged by the...|            negative|[encouraged, by, ...|\n",
      "|If you like origi...|            positive|[if, you, like, o...|\n",
      "|\"Phil the Alien i...|            negative|[\"phil, the, alie...|\n",
      "|I saw this movie ...|            negative|[i, saw, this, mo...|\n",
      "|\"So im not a big ...| meaning most of ...|[\"so, im, not, a,...|\n",
      "|The cast played S...|            negative|[the, cast, playe...|\n",
      "|This a fantastic ...|            positive|[this, a, fantast...|\n",
      "|Kind of drawn in ...|            negative|[kind, of, drawn,...|\n",
      "|Some films just s...|            positive|[some, films, jus...|\n",
      "|This movie made i...|            negative|[this, movie, mad...|\n",
      "|I remember this f...|            positive|[i, remember, thi...|\n",
      "|An awful film! It...|            negative|[an, awful, film!...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Tokenizer\n",
       "tkn: org.apache.spark.ml.feature.Tokenizer = tok_b91ad610c05a\n",
       "tokenized: org.apache.spark.sql.DataFrame = [review: string, sentiment: string ... 1 more field]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "\n",
    "val tkn = new Tokenizer()\n",
    "    .setInputCol(\"review\")\n",
    "    .setOutputCol(\"review_toks\")\n",
    "val tokenized = tkn.transform(df)\n",
    "tokenized.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592ca9f",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8d8d56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|              review|           sentiment|         review_toks|      review_toks_sw|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|One of the other ...|            positive|[one, of, the, ot...|[one, reviewers, ...|\n",
      "|\"A wonderful litt...| not only is it w...|[\"a, wonderful, l...|[\"a, wonderful, l...|\n",
      "|\"I thought this w...| but spirited you...|[\"i, thought, thi...|[\"i, thought, won...|\n",
      "|Basically there's...|            negative|[basically, there...|[basically, famil...|\n",
      "|\"Petter Mattei's ...| power and succes...|[\"petter, mattei'...|[\"petter, mattei'...|\n",
      "|\"Probably my all-...| but that only ma...|[\"probably, my, a...|[\"probably, all-t...|\n",
      "|I sure would like...|            positive|[i, sure, would, ...|[sure, like, see,...|\n",
      "|This show was an ...|            negative|[this, show, was,...|[show, amazing,, ...|\n",
      "|Encouraged by the...|            negative|[encouraged, by, ...|[encouraged, posi...|\n",
      "|If you like origi...|            positive|[if, you, like, o...|[like, original, ...|\n",
      "|\"Phil the Alien i...|            negative|[\"phil, the, alie...|[\"phil, alien, on...|\n",
      "|I saw this movie ...|            negative|[i, saw, this, mo...|[saw, movie, 12, ...|\n",
      "|\"So im not a big ...| meaning most of ...|[\"so, im, not, a,...|[\"so, im, big, fa...|\n",
      "|The cast played S...|            negative|[the, cast, playe...|[cast, played, sh...|\n",
      "|This a fantastic ...|            positive|[this, a, fantast...|[fantastic, movie...|\n",
      "|Kind of drawn in ...|            negative|[kind, of, drawn,...|[kind, drawn, ero...|\n",
      "|Some films just s...|            positive|[some, films, jus...|[films, simply, r...|\n",
      "|This movie made i...|            negative|[this, movie, mad...|[movie, made, one...|\n",
      "|I remember this f...|            positive|[i, remember, thi...|[remember, film,i...|\n",
      "|An awful film! It...|            negative|[an, awful, film!...|[awful, film!, mu...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StopWordsRemover\n",
       "englishStopWords: Array[String] = Array(i, me, my, myself, we, our, ours, ourselves, you, your, yours, yourself, yourselves, he, him, his, himself, she, her, hers, herself, it, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, ver...\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "val englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "val stops = new StopWordsRemover()\n",
    "    .setStopWords(englishStopWords)\n",
    "    .setInputCol(\"review_toks\")\n",
    "    .setOutputCol(\"review_toks_sw\")\n",
    "\n",
    "val sw = stops.transform(tokenized)\n",
    "\n",
    "sw.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a088f86",
   "metadata": {},
   "source": [
    "## Computing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddbab7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               TFOut|              IDFOut|\n",
      "+--------------------+--------------------+\n",
      "|(10000,[157,281,2...|(10000,[157,281,2...|\n",
      "|(10000,[157,1077,...|(10000,[157,1077,...|\n",
      "|(10000,[55,88,157...|(10000,[55,88,157...|\n",
      "|(10000,[144,157,1...|(10000,[144,157,1...|\n",
      "|(10000,[566,620,1...|(10000,[566,620,1...|\n",
      "|(10000,[281,331,8...|(10000,[281,331,8...|\n",
      "|(10000,[24,42,65,...|(10000,[24,42,65,...|\n",
      "|(10000,[78,370,53...|(10000,[78,370,53...|\n",
      "|(10000,[1,167,332...|(10000,[1,167,332...|\n",
      "|(10000,[78,1277,1...|(10000,[78,1277,1...|\n",
      "|(10000,[167,274,2...|(10000,[167,274,2...|\n",
      "|(10000,[167,263,4...|(10000,[167,263,4...|\n",
      "|(10000,[150,234,3...|(10000,[150,234,3...|\n",
      "|(10000,[281,440,4...|(10000,[281,440,4...|\n",
      "|(10000,[734,815,1...|(10000,[734,815,1...|\n",
      "|(10000,[261,793,8...|(10000,[261,793,8...|\n",
      "|(10000,[164,167,4...|(10000,[164,167,4...|\n",
      "|(10000,[25,51,72,...|(10000,[25,51,72,...|\n",
      "|(10000,[91,261,29...|(10000,[91,261,29...|\n",
      "|(10000,[49,292,30...|(10000,[49,292,30...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
       "tf: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_f22010f16283, binary=false, numFeatures=10000\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_1e2fa8a5fb12\n",
       "tfdat: org.apache.spark.sql.DataFrame = [review: string, sentiment: string ... 3 more fields]\n",
       "tfIdfOut: org.apache.spark.sql.DataFrame = [review: string, sentiment: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
    "\n",
    "\n",
    "val tf = new HashingTF()\n",
    "    .setInputCol(\"review_toks_sw\")\n",
    "    .setOutputCol(\"TFOut\")\n",
    "    .setNumFeatures(10000)\n",
    "\n",
    "val idf = new IDF()\n",
    "    .setInputCol(\"TFOut\")\n",
    "    .setOutputCol(\"IDFOut\")\n",
    "    .setMinDocFreq(2)\n",
    "\n",
    "val tfdat = tf.transform(sw)\n",
    "\n",
    "val tfIdfOut = idf.fit(tfdat)\n",
    "    .transform(tfdat)\n",
    "\n",
    "tfIdfOut.select(\"TFOut\", \"IDFOut\").show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
